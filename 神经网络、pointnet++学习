1、多层感知机可类比为泰勒展开式
2、sklearn.datasets中的 make_classification方法，其中 random_state 参数为随机种子，可保证同一种子下数据相同
3、sklearn.pregrocessing 的 3 种常见用法（scaler.fit_transform(X)）：
  （1）StandardScaler：标准化，将数据变换为均值为 0，标准差为 1 的标准正态分布。
  （2）MinMaxScaler：归一化，将数据按比例缩放到 [0, 1] 范围内（或其他指定范围）。
  （3）RobustScaler：鲁棒缩放，通过中位数和四分位差进行缩放，鲁棒性强，适用于含有离群点的数据。
4、在很多框架中，包括 PyTorch，需要手动为每一层添加激活函数。网络本身并不自动插入激活函数，因此需要显式地在每一层后添加激活函数（例如 ReLU、Sigmoid 等），
   并通过 forward 方法定义它们的应用顺序。
5、▶全连接神经网络的简单部署流程：
  （1）定义class的成员变量：
        a、调用父类初始化函数（可简写：super().__init__()）
        b、定义连接层 fc1、fc2
        c、定义激活函数
        d、定义输出层处理方式（比如分类任务用softmax将logits转为概率分布）
  （2）定义class的成员方法：
        a、forward 方法，其中定义了数据如何通过网络进行计算和传递，最终得到输出
  （3）初始化模型，包括：
        a、设置输入层、隐藏层、输出层大小
        b、model实例化
        c、选择损失函数
        d、选择和定义优化器（需要传入model的可调参数）
  （4）

6、数据集中异常值检测：
  （1）空缺值检测：
  （2）离群点检测：
          a、IQR（四分位距）方法：
                调用 pd.DataFrame 容器的 data['A'].quantile(0.25) 方法计算数据的四分数、
                根据四分位数设定阈值、
                利用pandas容器的运算特性：运算结果覆盖原值，比如 > 返回 bool 覆盖原址
          b、Z-score（标准分数）方法
                调用 scipy库 的stats.zscore方法直接计算标准分数：Z-score = (数据点 - μ) / σ、
                设定阈值（一般为 ±3），超过此范围的数据视为异常值
          c、基于机器学习的方法（如 Isolation Forest）【注意：异常数据的比例最好给定，不建议用 auto】
                创建 Isolation Forest 模型：model = IsolationForest(contamination=0.1, random_state=42)  # contamination表示异常比例
                训练模型：model.fit(data)
                预测：data['outlier'] = model.predict(data)
                参数要求：X 必须是 二维数组 (2D array-like)，格式一般是：
                    NumPy 数组：np.array
                    Pandas DataFrame
                    Python 列表（嵌套列表）
                    SciPy 稀疏矩阵

7、关于42：这个数字最早来自一部科幻小说《银河系漫游指南》（The Hitchhiker's Guide to the Galaxy）。在书中，超级计算机用750万年计算出了宇宙终极问题的
   答案——42，但没人知道问题是什么😂
8、张量增添维度方法的两种用法：
    （1）y = x.unsqueeze(dim) 【注意，该方法是返回结果，不会修改 x 】
    （2）torch.unsqueeze(input, dim)
9、掩码张量————批量进行数值替换或更新：
        mask = dist < distance
        distance[mask] = dist[mask]    # 只把 mask==true 对应位置的元素替换掉，其他位置不变
10、x = torch.Tensor([1, 2, 3])
    y = torch.tensor([1, 2, 3])
    两者区别在于前者默认float32，后者自动推断，推荐后者；
11、torch的 max 方法会返回连个张量，第一个为“值”， 第二个为“索引”；
12、pytorch 高级索引：y = x[torch.arange(B), [每个批次要获取点的索引], :]；
13、张量形状修改：x.expand(-1, -1, N, -1)，其中，-1 表示不变；
    【注意事项】
        • 只能对大小为 1 的维度进行扩展，不能扩展其他大小的维度。
        • 扩展操作不会分配新的内存，只是改变了张量的视图
        • 不原地改变！！
14、pytorch张量的广播机制会对形状不同的张量进行对齐操作，为防止隐患，还在自行对齐吧；
15、torch.Tensor.nonzero() 方法返回一个二维张量，其中每一行代表一个非零元素的索引，【原张量有几个维度，就有几列】
    当加上参数：as_tuple=True 时，返回的为3个元组，每个元组实际为原二维向量的每一列！；
16、np.savetxt 方法不能保存超过2维的数组；
17、▶卷积神经网络的简单部署流程：
    （1）

18、▶▶两种神经网络学习总结：
    （1）模型本身只定义结构和前向传播逻辑
            无论是多层感知机（MLP）还是卷积神经网络（CNN），模型类的作用仅仅是：
        a、定义神经网络的层次结构（如全连接层、卷积层、池化层等）
        b、指定激活函数
        c、描述数据如何通过网络层逐步传递
            但模型本身并不涉及参数的具体调整规则
    （2）参数调整完全交由外部工具（loss、优化器等？）实现
            原因：神经网络的可微性统一了反向传播算法
    （3）解耦的核心：自动微分系统
         工作方式：PyTorch 使用的是动态计算图（Dynamic Computational Graph）：
                  1、每次前向传播，都会重新构建计算图。
                  2、计算图的每个节点都包含【以下内容仅为粗浅认识，因为DCG并非pytorch的一个简单存储工具，它本身已然庞大，准确概念无法短期总结】：
                       a、运算类型（加法、卷积等）
                       b、运算的输入
                       c、如何对输入求导数的规则
                  3、当你调用：
                              loss.backward()
                     PyTorch 会沿着计算图逆序遍历，自动调用每个节点的梯度函数。
    （4）optimizer 不需要额外传入loss计算后的梯度信息是因为：梯度的存储是直接挂在每一层的张量自己身上的！
         在这里，optimizer.step() 只有一个任务：
            👉 它遍历自己管理的张量列表
            👉 从每个张量的 .grad 里读取梯度
            👉 按照优化算法更新张量的值

19、神经网络输入数据的维度排列遵循——内聚特征优先（特征优先排列，空间分组维度延后），而非符合人类认知习惯的空间在前，貌似恰恰相反；
    原因：暂时没看懂，时间不够，搁置~
20、torch的 view() 和 flatten() 区别：
    🔷 使用 view 调整形状：将 x 的形状调整为 (4, 3)，得到张量 y。通过 data_ptr() 方法检查 x 和 y 是否共享内存，结果为 True，说明 view 返回的是原张量的
视图。
    🔷 使用 flatten 展平：将 x 展平为一维张量 z。同样检查 x 和 z 是否共享内存，结果为 False，说明 flatten 分配了新的内存。
21、在torch张量中，对指定维度进行操作：
    （1）指定的维度是 以该维度的每个“元素”作为运算对象
    （2）同时，保持该维度下 每个“元素”的 内部形状
    （3）例如：当指定维度在中间时，会选择子元素中处于同一位置的子子元素进行运算；
              当指定维度在最后时，运算是只在“局部”进行，不存在“相同位置”进行运算
22、torch张量的 unsqueeze 方法，在插入维度时，可以传入平时视为 “越界” 的索引：max+1 ！
23、神经网络貌似强制要求数据类型为float32；
24、torch_gpu 2.3.1 不支持 numpy 2.x，但似乎 cpu版本的 torch 很多都支持 numpy2.x；
25、在 PyTorch 中，nn.Module 的子类（如你自定义的模型）确实没有直接的“默认”device 属性。模型和张量的 device 是由它们分别决定的。也就是说，模型 和 输入数
据 必须显式地都被迁移到相同的设备上（CPU 或 GPU）;

#########################
### 笔记不小心断了QAQ~ ###
#########################


    




